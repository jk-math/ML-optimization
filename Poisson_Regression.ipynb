{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Poisson_Regression.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"_ODo85dE0wYe","executionInfo":{"status":"ok","timestamp":1615928184758,"user_tz":240,"elapsed":379,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x8xYG6ze0wYg"},"source":["# Poisson Regression\n","\n","This is an example of an application of Convex Optimization found in the [Convex Optimization textbook](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf) by Boyd and Vandenberghe. There is a great [online course](https://www.edx.org/course/convex-optimization) offered on EdX which I highly recommend. \n","\n","Suppose $Y$ is a Poisson random variable with mean $\\mu$:\n","\n","$$ P(Y=k) = \\frac{\\mu^k e^{-k}}{k!} $$\n","\n","Suppose the mean $\\mu$ is a linear combination of some explanatory variables:\n","\n","$$ \\mu = a^Tx + b$$\n","\n","and that we have some data $(x_i,y_i)$ for $i=1,2,...,m$ where $x_i \\in \\mathbb{R}^n$ is a vector of explanatory variable values and $y_i$ is the response variable. We want to find MLE's of the parameters $a$ and $b$.\n"]},{"cell_type":"markdown","metadata":{"id":"c4hAXf850wYh"},"source":["The log likelihood is\n","\n","$$ \\displaystyle\\sum_{i=1}^m\\left( y_i\\log(a^Tx_i+b) - (a^Tx_i+b) - \\log(y_i!) \\right)$$\n","\n","Instead of maximizing the above log likelihood, we will solve the problem of minimizing \n","\n","$$ \\displaystyle\\sum_{i=1}^m a^Tx_i+b - y_i\\log(a^Tx_i+b)$$"]},{"cell_type":"markdown","metadata":{"id":"dFcKYjWp0wYh"},"source":["We will compile all of the observed data into a single design matrix, where we also include a column of ones for the intercept $b$. We wish to minimize\n","\n","$$ \\displaystyle\\sum_{i=1}^m \\left[(Xz)_i - y_i \\log((Xz)_i) \\right] $$\n","\n","where $X$ is the matrix whose rows are $x_1, x_2, ..., x_m$, with an additional column of ones for the intercept."]},{"cell_type":"markdown","metadata":{"id":"f7wg8Acd0wYh"},"source":["**Note:** In many other textbooks, Poisson regression is stated as follows. \n","\n","$$E\\left[Y|x\\right] = \\mu \\qquad \\text{ and } \\qquad \\log(\\mu) = Xz$$\n","\n","where $z$ is again the parameter vector we wish to estimate. So the assumption is the log of the expected value is a linear function of the parameter vector $z$. "]},{"cell_type":"code","metadata":{"scrolled":false,"id":"ZL_p0CyU0wYi","executionInfo":{"status":"ok","timestamp":1615928186675,"user_tz":240,"elapsed":291,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["# generate test data\n","n = 4 # number of explanatory variables\n","m = 20000 # number of data points\n","real_z = np.append(10*np.random.uniform(-1,1,n), 50) # last entry of z represents the intercept\n","# design matrix\n","X = np.concatenate([np.random.beta(2,5,(m,1)), np.random.normal(1,0.25, (m,1)), \n","                    np.random.binomial(1,0.7,(m,1)), np.random.uniform(1,2,(m,1)),\n","                   np.ones((m,1))], axis=1)\n","real_mu = X@real_z\n","y = np.random.poisson(real_mu, m)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmWbjM4b0wYi","executionInfo":{"status":"ok","timestamp":1615928189381,"user_tz":240,"elapsed":355,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"383b5697-50e5-4ce0-c2c1-a7e93f250f19"},"source":["real_z"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 2.2139755 ,  9.78492775, -7.12339699,  7.84141099, 50.        ])"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"hoi4FKhg0wYi","executionInfo":{"status":"ok","timestamp":1615928197469,"user_tz":240,"elapsed":415,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["def nll(X,y,z):\n","    return np.sum((X@z)-y*np.log(X@z))\n","def grad(X,y,z):\n","    return X.T@(1 -y/(X@z))\n","def backtrack(obj,X,y,z, gradient, alpha, beta):\n","    t = 1\n","    while np.min(X@(z-t*gradient)) <= 0 :\n","        t = t*beta\n","    while obj(X,y,z - t*gradient) >  obj(X,y,z) - alpha*t*gradient@gradient:\n","        t = t*beta\n","    return t\n","def l2_norm(x):\n","    return np.sqrt(np.sum(x**2))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TpgmSgls0wYj","executionInfo":{"status":"ok","timestamp":1615928223828,"user_tz":240,"elapsed":23689,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["# Gradient Descent\n","z = np.append(np.zeros(n),0.1) # initial point\n","tol = 1e-8 # stopping criterion tolerance for L2 norm of gradient\n","max_iter = 1000\n","# parameters \n","alpha = 0.1 # line search\n","beta = 0.5 # line search\n","i = 0\n","while i <= max_iter:\n","    g = grad(X,y,z)\n","    if l2_norm(g) < tol:\n","        break\n","    else:\n","        t = backtrack(nll, X, y, z, g, alpha, beta) # step length\n","        z = z - t*g # update\n","        i += 1"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"GKR9_mV10wYj","executionInfo":{"status":"ok","timestamp":1615928223835,"user_tz":240,"elapsed":21015,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"df215a9c-3ce9-482b-b92e-74df9ced05b8"},"source":["z"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 2.33707539,  9.7731621 , -7.22004301,  8.00166524, 49.80154963])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"iwchxuqi0wYk","executionInfo":{"status":"ok","timestamp":1615928223837,"user_tz":240,"elapsed":7402,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["def hessian(X,y,z):\n","    return X.T@np.diag(y/((X@z)**2))@X"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"vWOtTssP0wYk","executionInfo":{"status":"ok","timestamp":1615928223839,"user_tz":240,"elapsed":3887,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["def NM_backtrack(obj,X,y,z, gradient, newton_step, alpha, beta):\n","    t = 1\n","    while np.min(X@(z+t*newton_step)) <= 0 :\n","        t = t*beta\n","    while obj(X,y,z+t*newton_step) >  obj(X,y,z) + alpha*t*gradient@newton_step:\n","        t = t*beta\n","    return t"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oxi3RI_I0wYk","executionInfo":{"status":"ok","timestamp":1615928281985,"user_tz":240,"elapsed":31934,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["# Newton's Method\n","z = np.append(np.zeros(n),0.1)\n","tol = 1e-8 # stopping criterion tolerance for L2 norm of gradient\n","max_iter = 100\n","# parameters \n","alpha = 0.1 # line search\n","beta = 0.8 # line search\n","i = 0\n","while i <= max_iter:\n","    g = grad(X,y,z)\n","    H = hessian(X,y,z)\n","    newton_step = -np.linalg.inv(H)@g\n","    dec = -g@newton_step\n","    if dec < tol:\n","        break\n","    else:\n","        t = NM_backtrack(nll, X, y,z, g, newton_step, alpha, beta)\n","        z = z+t*newton_step\n","    i +=1"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DHczalZT0wYk","executionInfo":{"status":"ok","timestamp":1615928281991,"user_tz":240,"elapsed":29021,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"5c40c0f1-1795-45a9-9c01-415f35a39540"},"source":["z"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 2.33255879,  9.7700249 , -7.22062319,  7.99832   , 49.81167818])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMOAAUpq0wYl","executionInfo":{"status":"ok","timestamp":1615928283812,"user_tz":240,"elapsed":24051,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"b2838564-a03f-412d-b581-a938bc577ab0"},"source":["# confidence interval for parameter vector z\n","covar = np.linalg.inv(X.T@np.diag(X@z)@X)\n","lower_end = z -  2*np.exp(np.sqrt(np.diag(covar)))\n","upper_end = z +  2*np.exp(np.sqrt(np.diag(covar)))\n","for i in range(len(lower_end)):\n","    print('A 95% confidence interval for the', i,'th entry of the paramter vector is', round(lower_end[i],4), 'to', round(upper_end[i],4))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["A 95% confidence interval for the 0 th entry of the paramter vector is 0.3217 to 4.3435\n","A 95% confidence interval for the 1 th entry of the paramter vector is 7.7632 to 11.7769\n","A 95% confidence interval for the 2 th entry of the paramter vector is -9.2243 to -5.2169\n","A 95% confidence interval for the 3 th entry of the paramter vector is 5.9924 to 10.0043\n","A 95% confidence interval for the 4 th entry of the paramter vector is 47.7995 to 51.8239\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HUrJOmLr0wYl"},"source":[""],"execution_count":null,"outputs":[]}]}