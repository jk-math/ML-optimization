{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Huber_Regression.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"JrB77j_o2Y3_","executionInfo":{"status":"ok","timestamp":1615928689519,"user_tz":240,"elapsed":3576,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["import numpy as np\n","import cvxpy as cp\n","from sklearn.linear_model import HuberRegressor\n","from sklearn.preprocessing import StandardScaler\n","import plotly.graph_objects as go\n","import scipy"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yftn37Ad2Y4A"},"source":["# Huber Regression\n","\n","$$\\text{minimize  }  \\displaystyle\\sum_{i=1}^m\\phi(a_i^Tx-b_i) $$\n","\n","where $\\phi$ is the Huber penalty function and $a_i^T$ is the $i$th row of the given data matrix.\n","\n","$$\n","\\phi(x) = \\left\\{\n","     \\begin{array}{lr}\n","     \tu^2 &  \\text{if } |u| \\leq \\epsilon  \\\\\n","       \\epsilon(2|u|-\\epsilon) & u > \\epsilon\n","     \\end{array}\n","   \\right.\n","$$\n","\n","for some constant $M$. The objective function is differentiable, but it is not twice continuously differentiable. So, we can use the gradient descent method, but we cannot use Newton's method. Instead, we will implement a quasi-Newton method.\n","\n","We introduce a scale parameter $\\sigma$ and introduce some regularization and instead solve\n","\n","$$\\text{minimize  }  \\displaystyle\\sum_{i=1}^m\\left(\\sigma+ \\phi\\left(\\frac{a_i^Tx-b_i}{\\sigma}\\right) \\sigma\\right) + \\gamma \\|x\\|_2^2 $$"]},{"cell_type":"code","metadata":{"id":"QiDpgWa42Y4B","executionInfo":{"status":"ok","timestamp":1615928689522,"user_tz":240,"elapsed":3570,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["# generate some test data\n","m=200\n","n=10\n","A = np.random.normal(0,1,(m,n))\n","real_weights = np.random.rand(10)\n","b = A@real_weights + np.random.normal(0,0.25, m)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4V2VJ6X2Y4B"},"source":["## Gradient Descent\n","\n","We need to compute the gradient to implement the gradient descent algorithm. "]},{"cell_type":"code","metadata":{"id":"BAt7wXhS2Y4C","executionInfo":{"status":"ok","timestamp":1615928689524,"user_tz":240,"elapsed":3569,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["def l2_norm(x):\n","    return np.sqrt(np.sum(x**2))\n","def huber(x, eps):\n","    return np.where(np.abs(x)<=eps, x**2, 2*np.abs(x)*eps-eps**2)\n","def objective(x, A ,b, sigma,eps,gamma):\n","    return np.sum(sigma+huber((A@x-b)/sigma,eps)*sigma) + gamma*l2_norm(x)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcxNsIVx2Y4C","executionInfo":{"status":"ok","timestamp":1615928689526,"user_tz":240,"elapsed":3568,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["def huber_prime(x, eps):\n","    return np.where(np.abs(x)<=eps, 2*x, 2*eps*np.sign(x))\n","# last entry of vector is for variable sigma\n","def grad(x, A, b, sigma, eps, gamma):\n","    dx = A.T@huber_prime((A@x-b)/sigma, eps) + 2*gamma*x\n","    d_sigma = A.shape[0] + np.sum(huber((A@x-b)/sigma,eps)) - huber_prime((A@x-b)/sigma, eps)@((A@x-b)/sigma)\n","    return np.append(dx,d_sigma)\n","def neg_grad(x, A, b, sigma, eps, gamma):\n","    return -grad(x, A, b, sigma, eps, gamma)\n","# line search\n","def GD_backtrack(obj, x, A, b, sigma, eps, gamma, gradient, alpha, beta):\n","    t = 1\n","    # we need to make sure the scale parameter is >= 0\n","    while sigma-t*gradient[-1]<=0:\n","        t = t*beta\n","    while obj(x-t*gradient[:-1], A, b, sigma-t*gradient[-1], eps, gamma) > obj(x, A ,b, sigma,eps,gamma) - alpha*t*gradient@gradient:\n","        t = t*beta\n","    return t"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTLwn_tm2Y4D","executionInfo":{"status":"ok","timestamp":1615928694498,"user_tz":240,"elapsed":8538,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["# Gradient Descent\n","x = np.zeros(n) # initial point\n","sigma = 1.0\n","tol = 1e-5 # stopping criterion tolerance for L2 norm of gradient\n","max_iter = 1000\n","# parameters \n","eps = 1.35 # huber penalty\n","gamma = 0.0001 # regularization\n","alpha = 0.1 # line search\n","beta = 0.5 # line search\n","i = 0\n","while i <= max_iter:\n","    g = grad(x, A, b, sigma, eps, gamma)\n","    if l2_norm(g) < tol:\n","        break\n","    else:\n","        t = GD_backtrack(objective, x, A, b, sigma, eps, gamma, g, alpha, beta) # step length\n","        x = x - t*g[:-1] # update\n","        sigma = sigma - t*g[-1]\n","        i += 1"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oB-fVYGF2Y4D","executionInfo":{"status":"ok","timestamp":1615928694499,"user_tz":240,"elapsed":8531,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"2e50e1e6-0397-4c5b-80e2-f9a3454d7293"},"source":["x"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.12722972, 0.24303171, 0.86142117, 0.03919288, 0.16222664,\n","       0.69778093, 0.46308323, 0.11840425, 0.10479807, 0.9011498 ])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGF0Cu7M3SXz","executionInfo":{"status":"ok","timestamp":1615928834540,"user_tz":240,"elapsed":497,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"f05044d8-dd85-4321-e007-07b38fd7cf94"},"source":["real_weights"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.12648281, 0.2602493 , 0.88087601, 0.0340526 , 0.15545873,\n","       0.70199606, 0.44709851, 0.13121538, 0.12740934, 0.92702782])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBtiGB7Y2Y4E","executionInfo":{"status":"ok","timestamp":1615928694499,"user_tz":240,"elapsed":8524,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"2ca87557-f51e-4f75-f69c-bdb7a2611340"},"source":["sigma"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.1672287454071263"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3OuhNh232Y4E","executionInfo":{"status":"ok","timestamp":1615928694500,"user_tz":240,"elapsed":8518,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"3832ced0-2e69-4a84-b9b4-fb59f84b9309"},"source":["hr = HuberRegressor().fit(A,b)\n","hr.coef_"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.12751572, 0.2425425 , 0.86010154, 0.03914465, 0.15856361,\n","       0.69529159, 0.46179127, 0.11726195, 0.10467906, 0.90067322])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"f8kAKTx82Y4E","executionInfo":{"status":"ok","timestamp":1615928694500,"user_tz":240,"elapsed":8511,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"1f50921b-bcc4-4881-92ea-c88a8ac05230"},"source":["hr.scale_"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.16983428654474925"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"geF7Kqtn2Y4F"},"source":["## Quasi-Newton Method\n","\n","Since the huber penalty function is not twice continuously differentiable, we cannot implement Newton's Method. Instead, we use a Quasi-Newton method. In particular, we will follow the BFGS algorithm."]},{"cell_type":"code","metadata":{"id":"x_ANNjHX2Y4F","executionInfo":{"status":"ok","timestamp":1615928694501,"user_tz":240,"elapsed":8510,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["def QN_backtrack(obj, x, A, b, sigma, eps, gamma, gradient, step, alpha, beta):\n","    t = 1\n","    # we need to make sure the scale parameter is >= 0\n","    while sigma+t*step[-1]<=0:\n","        t = t*beta\n","    while obj(x+t*step[:-1], A, b, sigma+t*step[-1], eps, gamma) > obj(x, A ,b, sigma,eps,gamma) + alpha*t*gradient@step:\n","        t = t*beta\n","    return t\n","def bfgs(H, s, y):\n","    return H + (y@y.T)/(y.T@s)- (H@s@s.T@H)/(s.T@(H@s))"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcBx_JUw2Y4F","executionInfo":{"status":"ok","timestamp":1615928694501,"user_tz":240,"elapsed":8508,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":["# Quasi-Netwon Method\n","x = np.zeros(n) # initial point\n","sigma = 1.0\n","H = np.eye(n+1)\n","tol = 1e-5 \n","max_iter = 100\n","# parameters \n","eps = 1.35 # huber penalty\n","gamma = 0.0001 # regularization\n","alpha = 0.1 # line search\n","beta = 0.5 # line search\n","s, y = np.append(x,sigma), grad(x, A, b, sigma, eps, gamma)\n","g = grad(x, A, b, sigma, eps, gamma)\n","i = 0\n","diff = np.inf\n","while i <= max_iter: # max number of iterations as stopping criterion\n","    if diff < tol: # stop when the difference between consecutive objective values is \"small\"\n","        break\n","    else:\n","        diff = objective(x, A ,b, sigma,eps,gamma)\n","        # quasi-newton direction\n","        qn_step = -np.linalg.inv(H)@g\n","        # line search for step length\n","        t = QN_backtrack(objective, x, A, b, sigma, eps, gamma, g, qn_step, alpha, beta)\n","        # update\n","        x = x+t*qn_step[:-1]\n","        sigma = sigma +t*qn_step[-1]\n","        # set up for bfgs update of Hessian\n","        diff = np.abs(objective(x, A ,b, sigma,eps,gamma) - diff)\n","        g = grad(x, A, b, sigma, eps, gamma)\n","        w = np.append(x,sigma)\n","        x_diff = w - s\n","        grad_diff = g - y\n","        grad_diff = grad_diff.reshape(n+1,1)\n","        x_diff = x_diff.reshape(n+1,1)\n","        H = bfgs(H,x_diff,grad_diff)\n","        s = w\n","        y = g\n","        i += 1"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5r8QinsA2Y4G","executionInfo":{"status":"ok","timestamp":1615928694501,"user_tz":240,"elapsed":8501,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"83d3272d-71ca-4b00-8bc3-1177dd8afb5c"},"source":["x"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.17841387, 0.15405669, 0.68692868, 0.15767734, 0.20393744,\n","       0.63761634, 0.37310293, 0.16572161, 0.09275366, 0.86147259])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7BDvaJT2Y4G","executionInfo":{"status":"ok","timestamp":1615928694502,"user_tz":240,"elapsed":8495,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}},"outputId":"fcf79365-7d01-4f19-9fc5-3e85150b1448"},"source":["sigma"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.0511776667694443e-08"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"ObfgDmwA2Y4G","executionInfo":{"status":"ok","timestamp":1615928694502,"user_tz":240,"elapsed":8493,"user":{"displayName":"James Kreinbihl","photoUrl":"","userId":"17756256578181449259"}}},"source":[""],"execution_count":13,"outputs":[]}]}